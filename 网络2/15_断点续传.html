<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Document</title>
</head>

<style>
  #message {
    display: none;
    box-shadow: rgba(0, 0, 0, 0.3) 1px 1px 8px 4px;
    padding-top: 100px;
    height: 500px;
    background-color: #f1f1f1;
    margin-top: 20px;
    padding: 20px;
    border-radius: 10px;
  }
</style>
<body>
  <button id="btn">上传大文件</button>
  <input id="ipt" type="file" style="display: none;">
  <div id="message">
    <div>读取中请稍后...</div>
    <div class="data"></div>
  </div>

  <script>
    const btn = document.getElementById('btn')
    const input = document.getElementById('ipt')
    const message = document.getElementById('message')
    const data = document.querySelector('.data')

    btn.onclick = function () {
      input.click()
    }
    input.onchange = async function () {
      console.log("读取中请稍后...")
      message.style.display = 'block'

      const res = await splitFile(input.files[0])
      console.log(res, 'res')
      let li = ''
      li += `<li>文件的MD5：${res.fileId}</li>`
      li += `<li>文件后缀名：${res.suffix}</li>`
      for (const chunk of res.chunks) {
        li += `<li>分片的MD5：${chunk.id}</li>`
      }
      data.innerHTML = li
    }

    /**
     * 分片上传
     * 整体来说，实现断点上传的主要思路就是把要上传的文件切分为多个小的数据块然后进行上传
     * 虽然标准的不同，这也就意味着分片上传需要自行编写代码实现
     * @param {File} file 
     * @returns Promise
     */
    async function splitFile(file) {
      return new Promise((resolve, reject) => {
        // 分片尺寸(1M)，自定义
        const chunkSize = 1024 * 1024
        // 分片数量: 文件总大小 / 分片的大小
        const chunkCount = Math.ceil(file.size / chunkSize)
        // 当前 chunk 的下标
        let chunkIndex = 0
        // 存储每片的数据
        const chunks = []
        // 使用 ArrayBuffer 完成文件 MD5 编码
        const spark = new SparkMD5.ArrayBuffer()
        // 文件读取器，读取文件数据
        const fileReader = new FileReader()
        // 数据读取完成
        fileReader.onload = (e) => {
          // 每一片的 buffer
          const buffer = e.target.result
          // 分片数据追加到文件 MD5 中
          spark.append(buffer)
          // 当前分片的 MD5。加上 chunkIndex 的原因是唯一性。可能分片有重复，导致每片的 MD5 一样，这个问题这里暂时不处理(分片重用)
          const chunkMD5 = chunkIndex + "：" + SparkMD5.ArrayBuffer.hash(buffer)
          chunks.push({
            id: chunkMD5,
            content: new Blob([buffer])
          })
          chunkIndex++
          if (chunkIndex < chunkCount) {
            // 读取下一个分片
            loadNext()
          } else {
            // 已经读取完成
            const fileId = spark.end() // 整个文件的 MD5
            resolve({
              fileId,
              chunks,
              suffix: getSuffix(file.name)
            })
          }
        }
        // 读取下一个分片
        function loadNext() {
          const start = chunkIndex * chunkSize
          const end = start + chunkSize >= file.size ? file.size : start + chunkSize
          fileReader.readAsArrayBuffer(file.slice(start, end))
        }
        loadNext()

        function getSuffix(name) {
          const idx = name.lastIndexOf('.')
          if (idx !== -1) {
            return name.slice(idx)
          }
          return ''
        }
      })
    }
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/spark-md5/3.0.2/spark-md5.min.js"></script>
</body>
</html>